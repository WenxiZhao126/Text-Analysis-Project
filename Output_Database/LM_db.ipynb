{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wso0f_Smc4ee"
   },
   "source": [
    "# Synpsis\n",
    "\n",
    "Corpus importer for Jane Austen's *Persuasion*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SEDIrQRJc-nn"
   },
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "477MrZfFc9oh"
   },
   "outputs": [],
   "source": [
    "body_start = 684\n",
    "body_end = 67303\n",
    "chap_pat = r'^\\s*(?:VOLUME|BOOK|CHAPTER).*$'\n",
    "para_pat = r'\\n\\n+'\n",
    "sent_pat = r'([.;?!\"“”]+)'\n",
    "token_pat = r'([\\W_]+)'\n",
    "db_file = 'Les_Misérables.db'\n",
    "src_file_name = 'data/Les_Misérables.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "477MrZfFc9oh"
   },
   "outputs": [],
   "source": [
    "extra_stopwords = \"\"\"\n",
    "us rest went least would much must long one like much say well without though yet might still upon\n",
    "done every rather particular made many previous always never thy thou go first oh thee ere ye came\n",
    "almost could may sometimes seem called among another also however nevertheless even way one two three\n",
    "ever put\n",
    "\"\"\".strip().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "477MrZfFc9oh"
   },
   "outputs": [],
   "source": [
    "set(extra_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "JkFioCMVW8pl"
   },
   "outputs": [],
   "source": [
    "OHCO = ['chap_num', 'para_num', 'sent_num', 'token_num']\n",
    "CHAPS = OHCO[:1]\n",
    "PARAS = OHCO[:2]\n",
    "SENTS = OHCO[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A58rgKXydL8y"
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8617,
     "status": "ok",
     "timestamp": 1550542848134,
     "user": {
      "displayName": "Rafael Alvarado",
      "photoUrl": "https://lh3.googleusercontent.com/-gvKWs7zR4JY/AAAAAAAAAAI/AAAAAAABqfk/Q8O12g6M_T4/s64/photo.jpg",
      "userId": "11010075019714369526"
     },
     "user_tz": 300
    },
    "id": "YbadDvzWW0Sw",
    "outputId": "53c6fd6b-67cf-47cd-e43c-e46a8a334340"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('tagsets')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eC-m8q8Hd1dq"
   },
   "source": [
    "# Pragmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eC-m8q8Hd1dq"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AwIc1D23UuOQ"
   },
   "source": [
    "# Process\n",
    "\n",
    "We pause to look at the revised form of our text import function. The parsing function has been replaced with NLTK, which has improved the results of POS tagging. However, this has required some added string manipulation to produce better tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Oxgp19ejZmU"
   },
   "source": [
    "## Text to lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Oxgp19ejZmU"
   },
   "outputs": [],
   "source": [
    "lines = open(src_file_name, 'r', encoding='utf-8').readlines()\n",
    "lines = lines[body_start - 1 : body_end + 1]\n",
    "df = pd.DataFrame({'line_str':lines})\n",
    "df.index.name = 'line_id'\n",
    "del(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ScU67cbRdqoE"
   },
   "source": [
    "## Fix some characters to improve tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ScU67cbRdqoE"
   },
   "outputs": [],
   "source": [
    "df.line_str = df.line_str.str.replace('—', ' — ')\n",
    "df.line_str = df.line_str.str.replace('-', ' - ')\n",
    "df.line_str = df.line_str.str.replace(\"'\", \" ' \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b_raVDc5dsa4"
   },
   "source": [
    "## Lines to Chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b_raVDc5dsa4"
   },
   "outputs": [],
   "source": [
    "chap_mask = df.line_str.str.match(chap_pat)\n",
    "df.loc[chap_mask, 'chap_id'] = df.apply(lambda x: x.name, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nZD56huvGCzE"
   },
   "outputs": [],
   "source": [
    "df.chap_id = df.chap_id.ffill().astype('int')\n",
    "chap_ids = df.chap_id.unique().tolist()\n",
    "df['chap_num'] = df.chap_id.apply(lambda x: chap_ids.index(x))\n",
    "chaps = df.groupby('chap_num')\\\n",
    "    .apply(lambda x: ''.join(x.line_str))\\\n",
    "    .to_frame()\\\n",
    "    .rename(columns={0:'chap_str'})\n",
    "del(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EcffE_kleR0f"
   },
   "source": [
    "## Chapters to Paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EcffE_kleR0f"
   },
   "outputs": [],
   "source": [
    "paras = chaps.chap_str.str.split(para_pat, expand=True)\\\n",
    "    .stack()\\\n",
    "    .to_frame()\\\n",
    "    .rename(columns={0:'para_str'})\n",
    "paras.index.names = PARAS\n",
    "paras.para_str = paras.para_str.str.strip()\n",
    "paras.para_str = paras.para_str.str.replace(r'\\n', ' ')\n",
    "paras.para_str = paras.para_str.str.replace(r'\\s+', ' ')\n",
    "paras = paras[~paras.para_str.str.match(r'^\\s*$')]\n",
    "del(chaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ATUd7_HdeXk5"
   },
   "source": [
    "## Paragraphs to Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ATUd7_HdeXk5"
   },
   "outputs": [],
   "source": [
    "#     sents = paras.para_str.str.split(sent_pat, expand=True)\\\n",
    "sents = paras.para_str\\\n",
    "    .apply(lambda x: pd.Series(nltk.sent_tokenize(x)))\\\n",
    "    .stack()\\\n",
    "    .to_frame()\\\n",
    "    .rename(columns={0:'sent_str'})\n",
    "sents.index.names = SENTS\n",
    "del(paras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JglgExtWeaqN"
   },
   "source": [
    "## Sentences to Tokens with POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JglgExtWeaqN"
   },
   "outputs": [],
   "source": [
    "# tokens = sents.sent_str.str.split(token_pat, expand=True)\\\n",
    "tokens = sents.sent_str\\\n",
    "    .apply(lambda x: pd.Series(nltk.pos_tag(nltk.word_tokenize(x))))\\\n",
    "    .stack()\\\n",
    "    .to_frame()\\\n",
    "    .rename(columns={0:'pos_tuple'})\n",
    "tokens.index.names = OHCO\n",
    "tokens['pos'] = tokens.pos_tuple.apply(lambda x: x[1])\n",
    "tokens['token_str'] = tokens.pos_tuple.apply(lambda x: x[0])\n",
    "tokens = tokens.drop('pos_tuple', 1)\n",
    "del(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wZT8DGE7ek4F"
   },
   "source": [
    "## Tag punctuation and numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wZT8DGE7ek4F"
   },
   "outputs": [],
   "source": [
    "tokens['punc'] = tokens.token_str.str.match(r'^[\\W_]*$').astype('int')\n",
    "tokens['num'] = tokens.token_str.str.match(r'^.*\\d.*$').astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aH2pV_ABrkmB"
   },
   "source": [
    "## Extract vocab with minimal normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aH2pV_ABrkmB"
   },
   "outputs": [],
   "source": [
    "WORDS = (tokens.punc == 0) & (tokens.num == 0)\n",
    "tokens.loc[WORDS, 'term_str'] = tokens.token_str.str.lower()\\\n",
    "    .str.replace(r'[\"_*.]', '')\n",
    "vocab = tokens[tokens.punc == 0].term_str.value_counts().to_frame()\\\n",
    "    .reset_index()\\\n",
    "    .rename(columns={'index':'term_str', 'term_str':'n'})\n",
    "vocab = vocab.sort_values('term_str').reset_index(drop=True)\n",
    "vocab.index.name = 'term_id'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tKyrqpzfr7Le"
   },
   "source": [
    "## Get priors for Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tKyrqpzfr7Le"
   },
   "outputs": [],
   "source": [
    "vocab['p'] = vocab.n / vocab.n.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M9L8f3mBr8vW"
   },
   "source": [
    "## Add stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M9L8f3mBr8vW"
   },
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "vocab['port_stem'] = vocab.term_str.apply(lambda x: stemmer.stem(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W6Lyvd5Qr93-"
   },
   "source": [
    "## Define stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W6Lyvd5Qr93-"
   },
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words('english') + extra_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W6Lyvd5Qr93-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'also',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'an',\n",
       " 'and',\n",
       " 'another',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'called',\n",
       " 'came',\n",
       " 'can',\n",
       " 'could',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'done',\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'ere',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'few',\n",
       " 'first',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'go',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'however',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'least',\n",
       " 'like',\n",
       " 'll',\n",
       " 'long',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'made',\n",
       " 'many',\n",
       " 'may',\n",
       " 'me',\n",
       " 'might',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'much',\n",
       " 'must',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'never',\n",
       " 'nevertheless',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'oh',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 'particular',\n",
       " 'previous',\n",
       " 'put',\n",
       " 'rather',\n",
       " 're',\n",
       " 'rest',\n",
       " 's',\n",
       " 'same',\n",
       " 'say',\n",
       " 'seem',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'sometimes',\n",
       " 'still',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'thee',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'thou',\n",
       " 'though',\n",
       " 'three',\n",
       " 'through',\n",
       " 'thy',\n",
       " 'to',\n",
       " 'too',\n",
       " 'two',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 'upon',\n",
       " 'us',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'way',\n",
       " 'we',\n",
       " 'well',\n",
       " 'went',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'without',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'would',\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'ye',\n",
       " 'yet',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W6Lyvd5Qr93-"
   },
   "outputs": [],
   "source": [
    "sw = pd.DataFrame({'x':1}, index=stopwords)\n",
    "vocab['stop'] = vocab.term_str.map(sw.x).fillna(0).astype('int')\n",
    "del(sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WxRILKRNr-7C"
   },
   "source": [
    "## Add term_ids to Tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WxRILKRNr-7C"
   },
   "outputs": [],
   "source": [
    "tokens['term_id'] = tokens['term_str'].map(vocab.reset_index()\\\n",
    "    .set_index('term_str').term_id).fillna(-1).astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NTWfAutZKuRP"
   },
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xLQ1KGZCsMFx"
   },
   "outputs": [],
   "source": [
    "with sqlite3.connect(db_file) as db:\n",
    "    tokens.to_sql('token', db, if_exists='replace', index=True)\n",
    "    vocab.to_sql('vocab', db, if_exists='replace', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4q1kGXJuLDSp"
   },
   "outputs": [],
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TextmanCorpusImporter.ipynb",
   "provenance": [
    {
     "file_id": "1UJXtZFtWykmkbZSLyLxpKmwiGhXr1w6P",
     "timestamp": 1550255827012
    }
   ],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
